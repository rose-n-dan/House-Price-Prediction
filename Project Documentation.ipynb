{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predykcja cen nieruchomości oparta na modelu maszyny wektorów nośnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wstęp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem niniejszego projektu będzie zaimplemetowanie trzech regresorów służących do predykcji ceny domu, opartych na modelu maszyny wektorów nośnych (SVM), bazujących na kernelu:\n",
    "1. liniowym\n",
    "2. wielomianowym\n",
    "3. RBF. \n",
    "\n",
    "Dokumentacja będzie skupiała się na opisie rozwiązania problemu przewidywania cen nieruchomości z perspektywy potencjalnego kupującego oraz sprzedającego. Zebrane dane dotyczą obszarów położonych wokół amerykańskiego miasta Boston i pochodzą z 1974 roku. \n",
    "\n",
    "Model przewidujący cene rynkową nieruchomości mógłby być znakomitym narzędziem w rękach agenta, którego praca opiera się na analizach wartości domów. Z drugiej strony, potencjalny nabywca również mógłby wykorzystać model, aby zorientować się, w jakim przedziale cenowym nieruchomość, którą jest zainteresowany, się znajduje.\n",
    "\n",
    "Na cene domu, oprócz takich cech jak powierzchnia użytkowa i lokalizacja, wpływ ma wiele mniej oczywistych cech. W projektowanym modelu zostanie podjęta próba wykorzystania wszystkich użytecznych informacji w celu przeprowadzenia pełnej analizy i oszacowaniu cen nieruchomości.\n",
    "\n",
    "Cały etap projektowania modelu podzielony został na etapy, które umożliwią zoptymalizowanie danych pod kątem logicznym, tak, aby możliwe stało się ich jak najlepsze wykorzystanie w celach predykcji. Tymi krokami są:\n",
    "\n",
    "1. Zaimportowanie danych oraz bibliotek\n",
    "2. Analiza cechy predykowanej - ceny nieruchomości\n",
    "3. Analiza charakterystyk\n",
    "4. Imputacja brakujących danych oraz wyczyszczenie danych\n",
    "5. Optymalizacja danych\n",
    "6. Modelowanie i predykcja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Zaimportowanie danych oraz bibliotek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Zaimportowanie bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from scipy.stats import skew\n",
    "from scipy import stats\n",
    "from scipy.stats.stats import pearsonr\n",
    "from scipy.stats import norm\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LinearRegression,LassoCV, Ridge, LassoLarsCV,ElasticNetCV\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na początku zaimportowane zostaną wszystkie biblioteki, dzięki którym znaczącą ułatwiony zostanie proces analizowania danych. Wśród nich znajdują się biblioteki:\n",
    "\n",
    "1. Pandas - jest opensource'ową biblioteką, wydawaną na licencji BSD, która dostarcza zaawansowanych mechanizmów tworzenia struktur danych oraz ich analizy w języku Python;\n",
    "2. NumPy - jest podstawową bilblioteką wykorzystywaną w obliczeniach inżynierskich wykonywanych w Pythonie. Dostarcza mechanizmów tworzenia N-wymiarowych tablic, skomplikowanych funkcji matematycznych oraz integracji z kodami źródłowymi napisanymi w językach C/C++ oraz Fortran. Jednak najważniejszą funkcjonalnością pozostaje możliwość tworzenia kontenerów danych w celach ich późniejszej analizy.\n",
    "3. Seaborn - jest biblioteką, opartą o pakiet matplotlib, umożliwiającą wizualizacje danych i analiz w Pythonie na zaawansowanym poziomie\n",
    "4. matplotlib - ta biblioteka dostarcza mechanizmów rysowania gotowych do publikacji wykresów w wielu ogólnodostępnych formatach oraz w wielu środowiskach, w tym w wykorzystywanym w projekcie Jupyter Notebook.\n",
    "6. SciPy - dostarcza wielu mechanizmów, które umożliwiają na wykonanie często używanych operacji numerycznych, jak integracja, interpolacja, optymalizacja czy statystyka\n",
    "7. scikit-learn - jedna z najważniejszych bibliotek, która dostarcza mechanizmów sztucznej inteligencji w języku Python. Zawiera implmenetacje alogrytmów klasyfikacji, regresji oraz klasteryzacji opartych na metodach gradientowego wzmacniania regrecji, lasów losowych czy wykorzystywanych w projekcie maszynach wektorów nośnych. Dodatkowo jest kompatybilna i współpracuje razem z bibliotekami SciPy oraz NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Zaimportowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train and Test set\n",
    "train = pd.read_csv(\"./boston-housing/train.csv\")\n",
    "test = pd.read_csv(\"./boston-housing/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W dwóch plikach .csv zawarte są dane, które zostaną wykorzystane w celach budowy modeli regresorów. Dane pochodzą z repoztorium uczenia maszynowego UCI. Zostały zebrane w roku 1978 i zawierają 506 rekordów przedstawiających zagregowane informacje o nieruchomości oraz związanych z nią cechach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the numbers of samples and features\n",
    "print(\"The train data size before dropping Id feature is : {} \".format(train.shape))\n",
    "print(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n",
    "\n",
    "# Save the 'Id' column\n",
    "train_ID = train['ID']\n",
    "test_ID = test['ID']\n",
    "\n",
    "# Now drop the 'Id' column since it's unnecessary for the prediction process.\n",
    "train.drop(\"ID\", axis = 1, inplace = True)\n",
    "test.drop(\"ID\", axis = 1, inplace = True)\n",
    "\n",
    "# Check data size after dropping the 'Id' variable\n",
    "print(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \n",
    "print(\"The test data size after dropping Id feature is : {} \".format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że dane z pliku train.csv zawierają jedną kolumne więcej niż dane z pliku test.csv. Tą kolumną jest cena nieruchomości, którą model ma za zadanie przewidzieć."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cechami, które opisują każdą z nieruchomości w zaimportowanych danych są:\n",
    "1. crim - współczynnik przestępstw per capita w danym mieście\n",
    "2. zn - proporcja strefy gruntów mieszkalnych z mieszkaniami powyżej 25.000 stóp kwadratowych\n",
    "3. indus - proporcja działalności niehandlowwej w danym mieście\n",
    "4. chas - cecha określająca przyleganie nieruchomości do rzeki Charles (= 1 gdy przylega do rzeki, 0 w przeciwnym wypadku\n",
    "5. nox - koncentracja tlenku azotu (w cząsteczkach na 10 milionów)\n",
    "6. rm - średnia liczba pokoi na nieruchomość\n",
    "7. age - proporcja nieruchomości zajętych wybudowanych po 1940 roku\n",
    "7. dis - średnia ważona odległości od 5 największych bostońskich centrów zatrudnienia\n",
    "8. rad - index określający dostępność nieruchomości do autostrad\n",
    "9. tax - stopa podatku od nieruchomości per 10.000 dolarów\n",
    "10. ptratio - stosunek liczby uczniów do nauczycieli w danym mieście\n",
    "11. black - 1000(Bk - 0.63)^2 gdzie Bk to stosunek czarnoskórych w danym mieście\n",
    "12. lstat - procent populacji żyjący poniżej granicy ubóstwa\n",
    "13. medv - średnia wartość nieruchomości w 1000 dolarów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analiza cechy predykowanej - ceny nieruchomości "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Description\n",
    "train['medv'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Histogram\n",
    "sns.distplot(train['medv'] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['medv'])\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Median Value distribution')\n",
    "\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train['medv'], plot=plt)\n",
    "plt.show()\n",
    "\n",
    "print(\"Skewness: %f\" % train['medv'].skew())\n",
    "print(\"Kurtosis: %f\" % train['medv'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multivariable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix Heatmap\n",
    "corrmat = train.corr()\n",
    "f, ax = plt.subplots(figsize=(14, 8))\n",
    "sns.heatmap(corrmat, vmin=-1, vmax=1, square=True, center=0, annot=True, fmt='.2f');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = corrmat.abs().nlargest(14, 'medv')['medv'].index\n",
    "most_corr = pd.DataFrame(cols)\n",
    "most_corr.columns = ['Most Correlated Features']\n",
    "most_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLACKS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=train['black'], y=train['medv'], kind='reg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'black'\n",
    "data = pd.concat([train['medv'], train[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(25, 12))\n",
    "fig = sns.boxplot(x=var, y=\"medv\", data=data)\n",
    "fig.axis(ymin=0, ymax=52);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F A T A L N E** dane "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usuwamy je\n",
    "train.drop(['black'], axis=1, inplace=True)\n",
    "test.drop(['black'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usuwamy też rzekę - też lipna\n",
    "train.drop(['chas'], axis=1, inplace=True)\n",
    "test.drop(['chas'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dane o najwyższej wartości bezwzględnej korelacji**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=train['lstat'], y=train['medv'], kind='reg');\n",
    "print(train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers manually \n",
    "train = train.drop(train[(train['medv']>49.99) & (train['lstat']>8)].index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=train['lstat'], y=train['medv'], kind='reg');\n",
    "print(pearsonr(train['lstat'], train['medv']));\n",
    "print(train.shape[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=train['rm'], y=train['medv'], kind='reg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiekość pokoi może być różna - nie odrzucamy tu obserwacji odstających\n",
    "# train = train.drop(train[(train['medv'] > 8 * train['rm'] - 8)].index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.jointplot(x=train['rm'], y=train['medv'], kind='reg');\n",
    "# print(pearsonr(train['rm'], train['medv']));\n",
    "# print(train.shape[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=train['ptratio'], y=train['medv'], kind='reg');\n",
    "# w jednym mieście może być wiele regionów o różnej charakterystyce; \n",
    "# współczynnik ptratio jest wspólny dla wszystkich regionów miasta - biednych i bogatych\n",
    "# jednak można zauważyć silną ujemną korelację miedzy tymi wielkościami\n",
    "# ciężko cokolwiek odrzucić"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'ptratio'\n",
    "data = pd.concat([train['medv'], train[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(25, 12))\n",
    "fig = sns.boxplot(x=var, y=\"medv\", data=data)\n",
    "fig.axis(ymin=0, ymax=52);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=train['indus'], y=train['medv'], kind='reg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'indus'\n",
    "data = pd.concat([train['medv'], train[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(25, 12))\n",
    "fig = sns.boxplot(x=var, y=\"medv\", data=data)\n",
    "fig.axis(ymin=0, ymax=52);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=train['tax'], y=train['medv'], kind='reg');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'tax'\n",
    "data = pd.concat([train['medv'], train[var]], axis=1)\n",
    "f, ax = plt.subplots(figsize=(25, 12))\n",
    "fig = sns.boxplot(x=var, y=\"medv\", data=data)\n",
    "fig.axis(ymin=0, ymax=52);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uzupełnianie danych - nasze dane są kompletne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "train[\"medv\"] = np.log1p(train[\"medv\"])\n",
    "\n",
    "#Check the new distribution \n",
    "sns.distplot(train['medv'] , fit=norm);\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['medv'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train['medv'], plot=plt)\n",
    "plt.show()\n",
    "\n",
    "y_train = train.medv.values\n",
    "\n",
    "print(\"Skewness: %f\" % train['medv'].skew())\n",
    "print(\"Kurtosis: %f\" % train['medv'].kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "all_data.drop(['medv'], axis=1, inplace=True)\n",
    "print(\"Train data size is : {}\".format(train.shape))\n",
    "print(\"Test data size is : {}\".format(test.shape))\n",
    "print(\"Combined dataset size is : {}\".format(all_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
    "\n",
    "# Check the skew of all numerical features\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: (skew(x.dropna()))).sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'Positive skewed Features' :skewed_feats})\n",
    "skewness.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness = skewness[skewness > 0.8]\n",
    "# print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n",
    "skewness = skewness[skewness > 0.8]\n",
    "skewness = skewness.dropna()\n",
    "\n",
    "from scipy.special import boxcox1p\n",
    "\n",
    "skewed_features = skewness.index\n",
    "lam = 0.15\n",
    "for feat in skewed_features:\n",
    "    all_data[feat] = boxcox1p(all_data[feat], lam)\n",
    "    all_data[feat] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the skew of all numerical features\n",
    "skewed_feats = all_data[numeric_feats].apply(lambda x: (skew(x.dropna()))).sort_values(ascending=False)\n",
    "skewness = pd.DataFrame({'Positive skewed Features' :skewed_feats})\n",
    "skewness.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[:ntrain]\n",
    "test = all_data[ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
